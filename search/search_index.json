{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"In\u00edcio","text":""},{"location":"#ferramentas-utilizadas","title":"Ferramentas utilizadas","text":"<ul> <li>Python 3.11</li> <li>PySpark 3.5.3</li> <li>Delta Lake 3.2.0</li> <li>Apache Iceberg 0.5.0</li> <li>Poetry para gerenciamento do ambiente</li> <li>Jupyter Lab para os notebooks</li> <li>MkDocs para documenta\u00e7\u00e3o</li> </ul>"},{"location":"#estrutura","title":"Estrutura","text":"<ul> <li><code>notebooks/</code>: notebooks Jupyter com os testes e manipula\u00e7\u00e3o dos dados.</li> <li><code>data/</code>: dataset utilizado (<code>cidades_brasileiras.csv</code>)</li> <li><code>docs/</code>: documenta\u00e7\u00e3o da aplica\u00e7\u00e3o e exemplos com Delta e Iceberg.</li> <li><code>output/</code>: arquivos gerados com os dados processados.</li> </ul>"},{"location":"#dataset","title":"Dataset","text":"<p>O dataset cont\u00e9m dados p\u00fablicos de cidades brasileiras.</p>"},{"location":"#estrutura-dos-dados-csv","title":"Estrutura dos dados CSV","text":"<p>O arquivo CSV cont\u00e9m as seguintes colunas: - id - cidade - estado - sigla - ibge - latitude - longitude</p> <p>localizado em <code>data/cidades_brasileiras.csv</code>.</p>"},{"location":"#conteudo","title":"Conte\u00fado","text":"<ul> <li>Delta Lake</li> <li>Apache Iceberg</li> </ul>"},{"location":"delta/","title":"Delta Lake","text":"<p>Este documento descreve a cria\u00e7\u00e3o e manipula\u00e7\u00e3o de uma tabela Delta Lake com base no dataset <code>cidades_brasileiras.csv</code>.</p>"},{"location":"delta/#dataset","title":"Dataset","text":"<p>O arquivo CSV cont\u00e9m as seguintes colunas: - id - cidade - estado - sigla - ibge - latitude - longitude</p> <p>localizado em <code>data/cidades_brasileiras.csv</code>.</p>"},{"location":"delta/#modelo-er","title":"Modelo ER","text":"<p>A estrutura da tabela segue um modelo entidade-relacionamento simples, com uma \u00fanica entidade <code>CIDADES</code>, cujos atributos s\u00e3o:</p> <ul> <li><code>id</code>: Identificador \u00fanico da cidade (chave prim\u00e1ria)</li> <li><code>cidade</code>: Nome da cidade</li> <li><code>estado</code>: Nome do estado</li> <li><code>sigla</code>: Sigla do estado</li> <li><code>ibge</code>: C\u00f3digo IBGE da cidade</li> <li><code>latitude</code>: Latitude geogr\u00e1fica</li> <li><code>longitude</code>: Longitude geogr\u00e1fica</li> </ul>"},{"location":"delta/#configuracao-inicial-do-delta","title":"Configura\u00e7\u00e3o Inicial do DELTA","text":"<p>Faz a importa\u00e7\u00e3o correta do pyspark para ativar a sess\u00e3o SparkSession, al\u00e9m de importar os tipos de dados como String, Double, Int...</p> <p><pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n\nfrom delta import *\n\nimport logging\n\nlogging.getLogger(\"py4j\").setLevel(logging.DEBUG)\n</code></pre> Logo ap\u00f3s temos o builder inicial da Spark Session com as packages necess\u00e1rias</p> <pre><code>spark = (\n    SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n</code></pre>"},{"location":"delta/#criacao-do-schema-e-import-do-dataset","title":"Cria\u00e7\u00e3o do Schema e import do dataset","text":"<pre><code>schema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"cidade\", StringType(), True),\n    StructField(\"estado\", StringType(), True),\n    StructField(\"sigla\", StringType(), True),\n    StructField(\"ibge\", IntegerType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True)\n])\n\ndf = spark.read.csv(\"../data/cidades_brasileiras.csv\", header=True, schema=schema)\ndf.show(5)\n</code></pre>"},{"location":"delta/#criacao-da-tabela","title":"Cria\u00e7\u00e3o da Tabela","text":"<p><pre><code>df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"cidades_delta\")\nspark.sql(\"DROP TABLE IF EXISTS cidades_delta\")\nspark.sql(\"CREATE TABLE cidades_delta USING DELTA LOCATION '../output/delta/cidades'\")\n</code></pre>  Os dados s\u00e3o lidos de um arquivo CSV, convertidos em DataFrame com schema definido e salvos no formato Delta Lake. A tabela cidades_delta \u00e9 criada apontando para o diret\u00f3rio com os dados no formato Delta.</p>"},{"location":"delta/#insert","title":"INSERT","text":"<p><pre><code>spark.sql(\"\"\"\n    INSERT INTO cidades_delta VALUES\n    (9999, 'Cidade Exemplo', 'Estado Exemplo', 'EX', 9999999, -10.1234, -50.5678)\n\"\"\")\n\nspark.sql(\"SELECT * FROM cidades_delta WHERE id = 9999\").show()\n</code></pre> Este comando adiciona uma nova linha \u00e0 tabela Delta Lake, inserindo uma cidade fict\u00edcia no conjunto de dados existente.</p>"},{"location":"delta/#update","title":"UPDATE","text":"<pre><code>spark.sql(\"\"\"\n    UPDATE cidades_delta\n    SET latitude = -11.0000, longitude = -51.0000\n    WHERE id = 9999\n\"\"\")\n\nspark.sql(\"SELECT * FROM cidades_delta WHERE id = 9999\").show()\n</code></pre> <p>Atualiza o valor dos campos latitude e longitude na linha onde o id \u00e9 9999. Isso demonstra como realizar atualiza\u00e7\u00f5es em tabelas Delta usando comandos SQL.</p>"},{"location":"delta/#delete","title":"DELETE","text":"<pre><code>spark.sql(\"\"\"\n    DELETE FROM cidades_delta\n    WHERE id = 9999\n\"\"\")\n\nspark.sql(\"SELECT * FROM cidades_delta WHERE id = 9999\").show()\n</code></pre> <p>Remove a linha da tabela Delta onde o id \u00e9 igual a 9999, demonstrando a opera\u00e7\u00e3o de exclus\u00e3o de dados em uma tabela Delta.</p>"},{"location":"delta/#resultado-final","title":"Resultado Final","text":"<p><pre><code>df_final = spark.read.format(\"delta\").load(\"../output/delta/cidades\")\ndf_final.show()\n</code></pre> Carrega novamente os dados da tabela Delta em um dataframe final ap\u00f3s as opera\u00e7\u00f5es e exibe as primeiras linhas para verificar o estado final da tabela.</p>"},{"location":"iceberg/","title":"Apache Iceberg","text":"<p>Este documento descreve a cria\u00e7\u00e3o e manipula\u00e7\u00e3o de uma tabela Iceberg com base no dataset <code>cidades_brasileiras.csv</code>.</p>"},{"location":"iceberg/#dataset","title":"Dataset","text":"<p>O arquivo CSV cont\u00e9m as seguintes colunas: - id - cidade - estado - sigla - ibge - latitude - longitude</p> <p>localizado em <code>data/cidades_brasileiras.csv</code>.</p>"},{"location":"iceberg/#modelo-er","title":"Modelo ER","text":"<p>A estrutura da tabela segue um modelo entidade-relacionamento simples, com uma \u00fanica entidade <code>CIDADES</code>, cujos atributos s\u00e3o:</p> <ul> <li><code>id</code>: Identificador \u00fanico da cidade (chave prim\u00e1ria)</li> <li><code>cidade</code>: Nome da cidade</li> <li><code>estado</code>: Nome do estado</li> <li><code>sigla</code>: Sigla do estado</li> <li><code>ibge</code>: C\u00f3digo IBGE da cidade</li> <li><code>latitude</code>: Latitude geogr\u00e1fica</li> <li><code>longitude</code>: Longitude geogr\u00e1fica</li> </ul>"},{"location":"iceberg/#configuracao-incial-do-apache-iceberg","title":"Configura\u00e7\u00e3o incial do APACHE ICEBERG","text":"<p>Faz a importa\u00e7\u00e3o correta do pyspark para ativar a sess\u00e3o SparkSession, al\u00e9m de configurar o builder da SparkSession com os packages necess\u00e1rios</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder\n    .appName(\"IcebergExample\")\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.2\")\n    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n    .config(\"spark.sql.catalog.local.warehouse\", \"./output/iceberg-warehouse\")  \n    .getOrCreate()\n)\n</code></pre>"},{"location":"iceberg/#criacao-do-schema","title":"Cria\u00e7\u00e3o do Schema","text":"<p>Defini\u00e7\u00e3o do schema al\u00e9m da defini\u00e7\u00f5 dos tipos usados como Int, Double, String...</p> <pre><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"cidade\", StringType(), True),\n    StructField(\"estado\", StringType(), True),\n    StructField(\"sigla\", StringType(), True),\n    StructField(\"ibge\", IntegerType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True)\n])\n</code></pre> <p>Ap\u00f3s isso temos a leitura do csv para o dataframe <pre><code>df = spark.read.csv(\"../data/cidades_brasileiras.csv\", header=True, schema=schema)\ndf.show(5)\n</code></pre></p>"},{"location":"iceberg/#criacao-da-tabela","title":"Cria\u00e7\u00e3o da Tabela","text":"<pre><code>df.writeTo(\"local.cidades_iceberg\").using(\"iceberg\").createOrReplace()\n</code></pre>"},{"location":"iceberg/#insert","title":"INSERT","text":"<pre><code>spark.sql(\"\"\"\n    INSERT INTO local.cidades_iceberg VALUES\n    (9999, 'Cidade Exemplo', 'Estado Exemplo', 'EX', 9999999, -10.1234, -50.5678)\n\"\"\")\n\nspark.sql(\"SELECT * FROM local.cidades_iceberg WHERE id = 9999\").show()\n</code></pre> <p>Este comando adiciona uma nova linha \u00e0 tabela Iceberg, inserindo uma cidade fict\u00edcia no conjunto de dados existente.</p>"},{"location":"iceberg/#update","title":"UPDATE","text":"<pre><code>spark.sql(\"UPDATE local.cidades_iceberg SET cidade = 'Cidade Atualizada' WHERE id = 9999\")\nspark.sql(\"SELECT * FROM local.cidades_iceberg WHERE id = 9999\").show()\n</code></pre> <p>Atualiza o valor dos campos latitude e longitude na linha onde o id \u00e9 9999. Isso demonstra como realizar atualiza\u00e7\u00f5es em tabelas Iceberg usando comandos SQL.</p>"},{"location":"iceberg/#delete","title":"DELETE","text":"<pre><code>spark.sql(\"DELETE FROM local.cidades_iceberg WHERE id = 9999\")\nspark.sql(\"SELECT * FROM local.cidades_iceberg WHERE id = 9999\").show()\n</code></pre> <p>Remove a linha da tabela Iceberg onde o id \u00e9 igual a 9999, demonstrando a opera\u00e7\u00e3o de exclus\u00e3o de dados em uma tabela Iceberg.</p>"},{"location":"iceberg/#resultado-final","title":"Resultado Final","text":"<pre><code>df_final = spark.sql(\"SELECT * FROM local.cidades_iceberg\")\ndf_final.show(5)\n</code></pre> <p>Carrega novamente os dados da tabela Iceberg em um dataframe final ap\u00f3s as opera\u00e7\u00f5es e exibe as primeiras linhas para verificar o estado final da tabela.</p>"}]}